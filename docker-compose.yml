version: '3.7'

services:
  connector:
    image: "ghcr.io/flightaware/firestarter/firestarter_connector:${FS_VERSION:-latest}"
    build:
      context: .
      dockerfile: connector/Dockerfile
    init: true
    networks:
      - internal
    ports:
      # Provides optional access to the raw Firehose stream for consumption
      # by non-dockerized applications
      - "${STREAMING_PORT:-127.0.0.1:1601}:1601"
    environment:
      # REQUIRED environment variables
      # Firehose account username
      - FH_USERNAME=${FH_USERNAME:?FH_USERNAME variable must be set}
      # Firehose account key
      - FH_APIKEY=${FH_APIKEY:?FH_APIKEY variable must be set}
      # Use a single topic for all events to ensure proper ordering per flight
      - KAFKA_TOPIC_NAME=events

      # OPTIONAL environment variables
      # Firehose URL, defaults to firehose-test.flightaware.com.
      # firehose.flightaware.com can also be used
      - SERVER
      # Streaming compression of incoming Firehose data. Valid values are gzip,
      # deflate, or compress. Leave blank to disable compression.
      - COMPRESSION
      # Frequency in seconds to print stats about connection (messages/bytes
      # per second). Set to 0 to disable.
      - PRINT_STATS_PERIOD
      # Frequency in seconds that Firehose should send a synthetic "keepalive"
      # message to help connector ensure the connection is still alive. If no
      # such message is received within roughly $keepalive seconds, connector
      # will automatically reconnect to Firehose.
      - KEEPALIVE
      # The number of times that the same pitr seen in consecutive keeplive
      # messages should trigger an error and a restart of the connection
      - KEEPALIVE_STALE_PITRS
      # "Time mode" of Firehose init command. Can be "live" or "pitr <pitr>";
      # range is currently not supported.
      # See https://flightaware.com/commercial/firehose/documentation/commands
      # for more details.
      - INIT_CMD_TIME
      # Whether to resume from the last PITR committed. If set to "true", then
      # rather than consuming from what is specified in INIT_CMD_TIME, we first
      # look at the last record in each partition and use the largest PITR seen
      # in the initiation command. Defaults to "false".
      - RESUME_FROM_LAST_PITR
      # Timeout in seconds to use when resuming from last pitr. Defaults to 3 seconds
      # but can be any float value.
      - RESUME_FROM_LAST_PITR_TIMEOUT
      # Whether to produce keepalives to Kafka. Defaults to true. Set to "false" to
      # avoid producing these messages to Kafka.
      - KEEPALIVE_PRODUCE
      # The "optional" section of the Firehose init command. Mostly consists of
      # filters for the data. Do not put username, password, keepalive, or
      # compression commands here. Documentation at
      # https://flightaware.com/commercial/firehose/documentation/commands
      - INIT_CMD_ARGS

      # PYTHON settings
      - PYTHONUNBUFFERED=1
    logging:
      driver: "journald"
      options:
        tag: "connector"
    depends_on:
      - kafka

  summary_connector:
    image: "ghcr.io/flightaware/firestarter/firestarter_connector:${FS_VERSION:-latest}"
    build:
      context: .
      dockerfile: connector/Dockerfile
    init: true
    networks:
      - internal
    ports:
      # Provides optional access to the raw Firehose stream for consumption
      # by non-dockerized applications
      - "${STREAMING_PORT:-127.0.0.1:1602}:1601"
    environment:
      # REQUIRED environment variables
      # Firehose account username
      - FH_USERNAME=${FH_USERNAME:?FH_USERNAME variable must be set}
      # Firehose account key
      - FH_APIKEY=${FH_APIKEY:?FH_APIKEY variable must be set}
      # Use a single topic for all events to ensure proper ordering per flight
      - KAFKA_TOPIC_NAME=flights

      # OPTIONAL environment variables
      # Firehose URL, defaults to firehose-test.flightaware.com.
      # firehose.flightaware.com can also be used
      - SERVER
      # Streaming compression of incoming Firehose data. Valid values are gzip,
      # deflate, or compress. Leave blank to disable compression.
      - COMPRESSION
      # Frequency in seconds to print stats about connection (messages/bytes
      # per second). Set to 0 to disable.
      - PRINT_STATS_PERIOD
      # Frequency in seconds that Firehose should send a synthetic "keepalive"
      # message to help connector ensure the connection is still alive. If no
      # such message is received within roughly $keepalive seconds, connector
      # will automatically reconnect to Firehose.
      - KEEPALIVE
      # The number of times that the same pitr seen in consecutive keeplive
      # messages should trigger an error and a restart of the connection
      - KEEPALIVE_STALE_PITRS
      # "Time mode" of Firehose init command. Can be "live" or "pitr <pitr>";
      # range is currently not supported.
      # See https://flightaware.com/commercial/firehose/documentation/commands
      # for more details.
      - INIT_CMD_TIME
      # Whether to resume from the last PITR committed. If set to "true", then
      # rather than consuming from what is specified in INIT_CMD_TIME, we first
      # look at the last record in each partition and use the largest PITR seen
      # in the initiation command. Defaults to "false".
      - RESUME_FROM_LAST_PITR
      # Timeout in seconds to use when resuming from last pitr. Defaults to 3 seconds
      # but can be any float value.
      - RESUME_FROM_LAST_PITR_TIMEOUT
      # Whether to produce keepalives to Kafka. Defaults to true. Set to "false" to
      # avoid producing these messages to Kafka.
      - KEEPALIVE_PRODUCE
      # The "optional" section of the Firehose init command. Mostly consists of
      # filters for the data. Do not put username, password, keepalive, or
      # compression commands here. Documentation at
      # https://flightaware.com/commercial/firehose/documentation/commands
      - INIT_CMD_ARGS

      # PYTHON settings
      - PYTHONUNBUFFERED=1
    logging:
      driver: "journald"
      options:
        tag: "summary_connector"
    depends_on:
      - kafka

  db-updater:
    image: "ghcr.io/flightaware/firestarter/firestarter_db-updater:${FS_VERSION:-latest}"
    build:
      context: .
      dockerfile: db-updater/Dockerfile
    init: true
    networks:
      - internal
    environment:
      # URL to database that will be updated based on Firehose contents.
      # Documentation at https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls
      - DB_URL=${FLIGHTS_DB_URL:-sqlite:///db/flights.db}
      - PYTHONUNBUFFERED=1
      # Same kafka topic name as the producer of the feed that you want to consume
      - KAFKA_TOPIC_NAME=events
      # Consumers with the same group name will split the data between them,
      # but consumers with different group names will each receive all of the messages 
      - KAFKA_GROUP_NAME=group1
      # Topic to produce summarized flight data to
      - PRODUCER_TOPIC=flights-summaries
      # Cutoff in seconds after arrival to count a flight as completed
      - ARRIVAL_CUTOFF=25200
      # Set this to "flights" or "positions" depending on what kinds of messages this updater is handling
      - TABLE=flights
    volumes:
      - data:/home/firestarter/app/db
    logging:
      driver: "json-file"
      options:
        max-size: "10mb"
        max-file: "5"
    depends_on:
      - kafka

  db-summarizer:
    image: "ghcr.io/flightaware/firestarter/firestarter_db-summarizer:${FS_VERSION:-latest}"
    build:
      context: .
      dockerfile: db-summarizer/Dockerfile
    init: true
    networks:
      - internal
    environment:
      # URL to database that will be updated based on Firehose contents.
      # Documentation at https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls
      - DB_URL=${FLIGHTS_DB_URL:-sqlite:///db/flights.db}
      # Get print statements to the docker logs without buffering
      - PYTHONUNBUFFERED=1
      # Same kafka topic name as the producer of the feed that you want to consume
      - KAFKA_TOPIC_NAME=flights
      # Consumers with the same group name will split the data between them,
      # but consumers with different group names will each receive all of the messages 
      - KAFKA_GROUP_NAME=group1
      # How many hours after an update to purge a flight from the flights table
      - EXPIRATION_HOURS=18
      # Topic to produce summarized flight data to
      - PRODUCER_TOPIC_NAME=flights-summaries
      # Size of a batch of summarized flights when producing the PRODUCER_TOPIC
      - BATCH_SIZE=20000
    volumes:
      # The table is always flights
      - data:/home/firestarter/app/db
    logging:
      driver: "journald"
      options:
        tag: "db-summarizer"
    depends_on:
      - kafka

  position-db-updater:
    image: "ghcr.io/flightaware/firestarter/firestarter_db-updater:${FS_VERSION:-latest}"
    build:
      context: .
      dockerfile: db-updater/Dockerfile
    init: true
    networks:
      - internal
    environment:
      # URL to database that will be updated based on Firehose contents.
      # Documentation at https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls
      - DB_URL=${POSITIONS_DB_URL:-postgresql://postgres:positions@timescaledb:5432}
      - PYTHONUNBUFFERED=1
      # Same kafka topic name as the producer of the feed that you want to consume
      - KAFKA_TOPIC_NAME=events
      # Consumers with the same group name will split the data between them,
      # but consumers with different group names will each receive all of the messages 
      - KAFKA_GROUP_NAME=position_group1
      # Set this to "flights" or "positions" depending on what kinds of messages this updater is handling
      - TABLE=positions
    logging:
      driver: "json-file"
      options:
        max-size: "10mb"
        max-file: "5"
    depends_on:
      - kafka
      - timescaledb

  fids-backend:
    image: "ghcr.io/flightaware/firestarter/firestarter_fids:${FS_VERSION:-latest}"
    build:
      context: .
      dockerfile: fids/Dockerfile
    init: true
    networks:
      internal:
        aliases:
          - fids-backend
    environment:
      # URL to database that is being updated by db-updater.
      # Documentation at https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls
      - FLIGHTS_DB_URL=${FLIGHTS_DB_URL:-sqlite:///db/flights.db}
      - POSITIONS_DB_URL=${POSITIONS_DB_URL:-postgresql://postgres:positions@timescaledb:5432}
      - GOOGLE_MAPS_API_KEY=${GOOGLE_MAPS_API_KEY:-}
      - PYTHONUNBUFFERED=1
    volumes:
      - data:/home/firestarter/app/db
    logging:
      driver: "json-file"
      options:
        max-size: "10mb"
        max-file: "5"

  fids-frontend:
    image: "ghcr.io/flightaware/fids_frontend/fids-frontend:${FIDS_VERSION:-latest}"
    ports:
      # Port upon which to serve webapp
      - "${WEB_SERVER_PORT:-8080}:80"
    networks:
      - internal
    logging:
      driver: "json-file"
      options:
        max-size: "10mb"
        max-file: "5"

  map:
    image: "ghcr.io/flightaware/firestarter/firestarter_map:${FS_VERSION:-latest}"
    build:
      context: .
      dockerfile: map/Dockerfile
    init: true
    ports:
      # Port upon which to serve webapp
      - "${MAP_SERVER_PORT:-5001}:5001"
    networks:
      - internal
    environment:
      - PYTHONUNBUFFERED=1
      - KAFKA_TOPIC_NAME=events
      - KAFKA_GROUP_NAME=map_group
      - GOOGLE_MAPS_API_KEY
    logging:
      driver: "json-file"
      options:
        max-size: "10mb"
        max-file: "5"
    depends_on:
      - kafka

  s3-exporter:
    image: "ghcr.io/flightaware/firestarter/firestarter_s3-exporter:${FS_VERSION:-latest}"
    build:
      context: .
      dockerfile: s3-exporter/Dockerfile
    environment:
      # PYTHON settings for logging purposes
      - PYTHONUNBUFFERED=1

      # REQUIRED environment variables
      - S3_BUCKET=${S3_BUCKET:?S3_BUCKET variable must be set}

      # OPTIONAL environment variables
      # Bucket folder: keep blank to avoid writing files to a folder
      - S3_BUCKET_FOLDER
      # Kafka brokers (defaults to kafka:9092)
      - KAFKA_BROKERS
      # Kafka topic (defaults to events)
      - KAFKA_TOPIC
      # Max size of the asyncio queues used by the coroutines
      # Defaults to 0 which means there is not max size
      - ASYNCIO_QUEUE_MAX_SIZE
      # How to decide when a batch of records should be written to S3
      # Can be records, bytes, or both. Defaults to both
      - BATCH_STRATEGY
      # Max records per file written to S3
      # Defaults to 15k
      - RECORDS_PER_FILE
      # Max byes per file written to S3 
      # Defaults to 10MB
      # Once RECORDS_PER_FILE or BYTES_PER_FILE is reached, a file
      # is written to S3
      - BYTES_PER_FILE
      # Compression type. Can be none or bzip. Defaults to none
      - COMPRESSION_TYPE
      # Log level. Can be debug, info, warning, error, or critical
      - LOG_LEVEL

    init: true
    restart: unless-stopped
    networks:
      - internal
    volumes:
      # Map the docker-compose user's aws home directory
      # into the container although AWS credentials can also
      # be provided in environment variables
      # This is turned off by default, but comment out to provide
      # credentials this way rather than through environment variables
      - /data/.aws:/home/firestarter/.aws:ro
    logging:
      driver: "journald"
      options:
        tag: "s3-exporter"
    depends_on:
      - kafka


  s3-exporter-flights-summary:
    image: "ghcr.io/flightaware/firestarter/firestarter_s3-exporter:${FS_VERSION:-latest}"
    build:
      context: .
      dockerfile: s3-exporter/Dockerfile
    environment:
      # PYTHON settings for logging purposes
      - PYTHONUNBUFFERED=1

      # REQUIRED environment variables
      - S3_BUCKET=${S3_BUCKET:?S3_BUCKET variable must be set}

      # OPTIONAL environment variables
      # Bucket folder: keep blank to avoid writing files to a folder
      - S3_BUCKET_FOLDER=summary
      # Kafka brokers (defaults to kafka:9092)
      - KAFKA_BROKERS
      # Kafka topic (defaults to events)
      - KAFKA_TOPIC=flights-summaries
      # Max size of the asyncio queues used by the coroutines
      # Defaults to 0 which means there is not max size
      - ASYNCIO_QUEUE_MAX_SIZE
      # How to decide when a batch of records should be written to S3
      # Can be records, bytes, or both. Defaults to both
      - BATCH_STRATEGY=both
      # Max records per file written to S3
      # Defaults to 15k
      - RECORDS_PER_FILE=100000
      # Max byes per file written to S3 
      # Defaults to 10MB
      # Once RECORDS_PER_FILE or BYTES_PER_FILE is reached, a file
      # is written to S3
      - BYTES_PER_FILE=32MB
      # Compression type. Can be none or bzip. Defaults to none
      - COMPRESSION_TYPE=bzip
      # Log level. Can be debug, info, warning, error, or critical
      - LOG_LEVEL=info

    init: true
    restart: unless-stopped
    networks:
      - internal
    volumes:
      # Map the docker-compose user's aws home directory
      # into the container although AWS credentials can also
      # be provided in environment variables
      # This is turned off by default, but comment out to provide
      # credentials this way rather than through environment variables
      - /data/.aws:/home/firestarter/.aws:ro
    logging:
      driver: "journald"
      options:
        tag: "s3-exporter-flights-summary"
    depends_on:
      - kafka

  per-message-s3-exporter:
    image: "ghcr.io/flightaware/firestarter/firestarter_per-message-s3-exporter:${FS_VERSION:-latest}"
    build:
      context: .
      dockerfile: per-message-s3-exporter/Dockerfile
    environment:
      # PYTHON settings for logging purposes
      - PYTHONUNBUFFERED=1

      # S3 bucket to write data into
      - S3_BUCKET=${S3_BUCKET:?S3_BUCKET variable must be set}
      # Bucket folder prefix (optional)
      - S3_BUCKET_FOLDER
      # Firehose account username
      - FH_USERNAME=${FH_USERNAME:?FH_USERNAME variable must be set}
      # Firehose account key
      - FH_APIKEY=${FH_APIKEY:?FH_APIKEY variable must be set}

      # Max size of the asyncio queues used by the coroutines
      # Defaults to 0 which means there is not max size
      - ASYNCIO_QUEUE_MAX_SIZE
      # How to decide when a batch of records should be written to S3
      # Can be records, bytes, or both. Defaults to both
      - BATCH_STRATEGY
      # Max records per file written to S3
      # Defaults to 15k
      - RECORDS_PER_FILE
      # Max byes per file written to S3 
      # Defaults to 128MB
      - BYTES_PER_FILE
      # Message types are particularly voluminous
      # If set, allows for setting separate thresholds for these
      - COMMON_MESSAGE_TYPES
      # Separate thresholds for common message types 
      - BYTES_PER_FILE_COMMON_MESSAGE_TYPES
      - RECORDS_PER_FILE_COMMON_MESSAGE_TYPES
      # Compression type. Can be none or bzip. Defaults to none
      - COMPRESSION_TYPE
      # Log level. Can be debug, info, warning, error, or critical
      - LOG_LEVEL
      # AWS profile from credentials file
      - AWS_PROFILE

      # OPTIONAL environment variables
      # Firehose URL, defaults to firehose-test.flightaware.com.
      # firehose.flightaware.com can also be used
      - SERVER
      # Streaming compression of incoming Firehose data. Valid values are gzip,
      # deflate, or compress. Leave blank to disable compression.
      - COMPRESSION
      # Frequency in seconds to print stats about connection (messages/bytes
      # per second). Defaults to 10.
      - PRINT_STATS_PERIOD
      # Frequency in seconds that Firehose should send a synthetic "keepalive"
      # message to help connector ensure the connection is still alive. If no
      # such message is received within roughly $keepalive seconds, connector
      # will automatically reconnect to Firehose.
      - KEEPALIVE
      # The number of times that the same pitr seen in consecutive keeplive
      # messages should trigger an error and a restart of the connection
      - KEEPALIVE_STALE_PITRS
      # "Time mode" of Firehose init command. Can be "live" or "pitr <pitr>";
      # range is currently not supported.
      - INIT_CMD_TIME
      # The "optional" section of the Firehose init command. Mostly consists of
      # filters for the data. Do not put username, password, keepalive, or
      # compression commands here.
      - INIT_CMD_ARGS

    init: true
    restart: always
    networks:
      - internal
    volumes:
      # Map the docker-compose user's aws home directory
      # into the container although AWS credentials can also
      # be provided in environment variables
      # This is turned off by default, but comment out to provide
      # credentials this way rather than through environment variables
      - "${HOME}/.aws:/home/firestarter/.aws:ro"

      # Need a volume for the PITRs map 
      - pitrs_map:/home/firestarter/pitrs
    logging:
      driver: "journald"
      options:
        tag: "per-message-s3-exporter"

  zookeeper:
    image: "bitnami/zookeeper:3.6.2"
    init: true
    networks:
      - internal
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    volumes:
      - zookeeper_data:/bitnami/zookeeper
    logging:
      driver: "journald"
      options:
        tag: "zookeeper"

  kafka:
    image: "bitnami/kafka:3.1"
    init: true
    networks:
      - internal
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      # Retain messages for 6 hours
      - KAFKA_CFG_LOG_RETENTION_HOURS=6
    logging:
      driver: "journald"
      options:
        tag: "kafka"
    volumes:
      - kafka_data:/bitnami/kafka
    depends_on:
      - zookeeper

  timescaledb:
    image: "timescale/timescaledb:1.7.4-pg12"
    init: true
    networks:
      - internal
    environment:
      - POSTGRES_PASSWORD=positions
    volumes:
      - position_data:/var/lib/postgresql/data

volumes:
  data:
  kafka_data:
  zookeeper_data:
  position_data:
  pitrs_map:

networks:
  internal:
