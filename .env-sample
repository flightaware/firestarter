##
##
## connector
##
##
FH_USERNAME=user
FH_APIKEY=key
INIT_CMD_ARGS=events "flifo departure arrival cancellation position"

##
##
## maps
##
##
GOOGLE_MAPS_API_KEY=key

##
##
## s3-exporter
##
##
# Name of the S3 bucket to write files into
#S3_BUCKET=

# Name of the folder in the S3 bucket to use (if applicable)
#S3_BUCKET_FOLDER=

# AWS access key
#AWS_ACCESS_KEY_ID=

# AWS secret access key
#AWS_SECRET_ACCESS_KEY=

# Kafka bootstrap brokers URL to use (defaults to kafka:9092)
#KAFKA_BROKERS=kafka:9092

# Kafka topic to read records from (defaults to events)
#KAFKA_TOPIC=events

# asyncio Queue max size (defaults to 0 which means no max size)
# This can be used to slow down the s3-exporter service and ensure
# there isn't runaway memory use if the S3 writer is much slower than
# the coroutine that batches up records for writing
#ASYNCIO_QUEUE_MAX_SIZE=0

# Whether to write a batch of records to S3 based on whether a threshold
# number of records was reached, a threshold number of bytes, or whether
# either a threshold of records or bytes was reached
# Acceptable values are records, bytes, or both
# Defaults to both, which means once either threshold is reached a batch is
# written to a file in S3
#BATCH_STRATEGY=

# Maximum number of Kafka records to include in a single S3 file
# Defaults to 15k
#RECORDS_PER_FILE=15000

# Maximum number of bytes to include in a single S3 file (roughly speaking
# since we append a record to the current batch and only then check if the
# bytes threshold has been exceeded)
# Defaults to 10MB (can provide human sizes for bytes instead of raw integer
# amounts)
#BYTES_PER_FILE=10MB

# Compression type to use. Can be either none or bzip. Defaults to none
#COMPRESSION_TYPE=none

# Log level. s3-exporter only logs at INFO or higher. Defaults to info
# Can be debug, info, warning, error, or critical
#LOG_LEVEL=info
