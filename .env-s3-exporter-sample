# Name of the S3 bucket to write files into
S3_BUCKET=

# Name of the folder in the S3 bucket to use (if applicable)
#S3_BUCKET_FOLDER=

# Kafka bootstrap brokers URL to use (defaults to kafka:9092)
#KAFKA_BROKERS=

# Kafka topic to read records from (defaults to events)
#KAFKA_TOPIC=

# asyncio Queue max size (defaults to 0 which means no max size)
# This can be used to slow down the s3-exporter service and ensure
# there isn't runaway memory use if the S3 writer is much slower than
# the coroutine that batches up records for writing
#ASYNCIO_QUEUE_MAX_SIZE=

# Maximum number of Kafka records to include in a single S3 file
# Defaults to 15k
#RECORDS_PER_FILE=

# Maximum number of bytes to include in a single S3 file (roughly speaking
# since we append a record to the current batch and only then check if the
# bytes threshold has been exceeded)
# Defaults to 10MB (can provide human sizes for bytes instead of raw integer
# amounts)
#BYTES_PER_FILE=

# Compression type to use. Can be either none or bzip. Defaults to none
#COMPRESSION_TYPE=

# Log level. s3-exporter only logs at INFO or higher. Defaults to info
# Can be debug, info, warning, error, or critical
#LOG_LEVEL=
